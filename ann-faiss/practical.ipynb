{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Nearest Neighbor (ANN) Methods\n",
    "\n",
    "In this practical, we would like to compare exhaustive search to ANN methods\n",
    "\n",
    "  * Exhaustive Search, Product Quantization (PQ) and Vector Quantization\n",
    "\n",
    "  * Will only count the time/memory of retrieval (as set up times, e.g. for indexing, only have to be done once)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
     "## If you do not have data.tar.gz or lab.py, e.g., you're running this\n",
     "## on Google Colab, uncomment the following lines to download them.\n",
     "\n",
     "## data.tar.gz: the Sift1M dataset.\n",
     "# wget --no-verbose https://thor.robots.ox.ac.uk/practicals/ann-faiss-2021/data.tar.gz\n",
     "# echo \"b23d1b3b2ee8469d819b61ca900ef0ed  data.tar.gz\" | md5sum --check || exit 1 \n",
     "# tar --extract --gzip --file data.tar.gz\n",
     "\n",
     "## lab.py: Python module with utilities for the exercises.\n",
     "# wget --no-verbose https://github.com/ox-vgg/practicals/raw/main/ann-faiss/lab.py\n",
     "\n",
     "## Colab does not have Faiss, you need to install it, but Colab runs\n",
     "## Python 3.8 and Fais only provides binary releases for Python 3.7.\n",
     "## So you need to build Faiss from source.  Also, the latest Faiss\n",
     "## requires Swig 4 but Colab runs Ubuntu Bionic which has Swig 3.\n",
     "## You could also build Swig 4 from source but it's simpler to build\n",
     "## just build Faiss 1.7.1 which was the latest to support Swig 3.\n",
     "# apt-get install swig\n",
     "# wget --no-verbose https://github.com/facebookresearch/faiss/archive/refs/tags/v1.7.1.tar.gz\n",
     "# echo \"adbac84718323054642c3173bb7b57d7  v1.7.1.tar.gz\" | md5sum --check || exit 1\n",
     "# tar --extract --gzip --file v1.7.1.tar.gz\n",
     "# cmake -DFAISS_ENABLE_PYTHON=ON -DFAISS_ENABLE_GPU=OFF -S faiss-1.7.1 -B faiss-1.7.1/build\n",
     "# cmake --build faiss-1.7.1/build\n",
     "# cmake --install faiss-1.7.1/build\n",
     "# cd faiss-1.7.1/build/faiss/python && pip install .\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import time\n",
    "import faiss\n",
    "\n",
    "import lab\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start benchmarking\n",
    "\n",
    "This bring us to the three metrics of interest:\n",
    "\n",
    "- **Speed**. How long does it take to find the 10 (or some other number) most similar vectors to the query? Hopefully less time than the brute-force algorithm needs; otherwise, what’s the point of indexing?\n",
    "\n",
    "- **Memory usage**. How much RAM does the method require? More or less than the original vectors? Faiss supports searching only from RAM, as disk databases are orders of magnitude slower. Yes, even with SSDs.\n",
    "\n",
    "- **Accuracy**. How well does the returned list of results match the brute-force search results? Accuracy can be evaluated by counting the number of queries for which the true nearest neighbor is returned first in the result list (a measure called 1-recall@1), or by measuring the average fraction of 10 nearest neighbors that are returned in the 10 first results (the “10-intersection” measure).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Exhaustive Search NN\n",
    "\n",
    "def ESNN(xq, xb, k):\n",
    "    D = xb.shape[1] # size of the vectors\n",
    "    Nindex = faiss.IndexFlatL2(D) # build the index\n",
    "    Nindex.add(xb)                # add vectors to the index, no processing is applied to the vectors\n",
    "    start = time.time()\n",
    "    Ndist, Ni = Nindex.search(xq, k)  # actual search, Ndist: distance, Ni: the index of the NN\n",
    "    end = time.time()\n",
    "    NMem = lab.get_memory(Nindex)\n",
    "    return [end-start, NMem, Ndist, Ni]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Index Product Quantization\n",
    "\n",
    "def PQNN(xq, xb, k, m, nbits):\n",
    "  # m: number of subvectors you'd like to split\n",
    "  # nbits: number of bits per subquantizer, k_ = 2**nbits, i.e. number of centroids per sub-space\n",
    "  D = xb.shape[1] # size of the vectors\n",
    "  assert D % m == 0  # make sure it's divisible\n",
    "\n",
    "  PQindex = faiss.IndexPQ(D, m, nbits) # build the index\n",
    "  PQindex.train(xb)  # PQ training can take some time when using large nbits\n",
    "  PQindex.add(xb)\n",
    "\n",
    "  start = time.time()\n",
    "  PQdist, PQi = PQindex.search(xq, k)\n",
    "  end = time.time()\n",
    "  PQMem = lab.get_memory(PQindex)\n",
    "  return [end-start, PQMem, PQdist, PQi]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Index IVFPQ\n",
    "# To speed up our search time we can add another step, using an IVF index.\n",
    "\n",
    "def IVFPQNN(xq, xb, k=5, nlist=2048, nbits=8, nprobe=2):\n",
    "  # nlist: how many Voronoi cells\n",
    "  # nbits: when using IVF+PQ, can use 8, higher nbits values are not supported\n",
    "  # nprobe: number of centroids for searching\n",
    "\n",
    "  D = xb.shape[1] # size of the vectors\n",
    "  quantizer = faiss.IndexFlatL2(D)\n",
    "\n",
    "  IVFPQindex = faiss.IndexIVFPQ(quantizer, D, nlist, m, nbits)\n",
    "  IVFPQindex.train(xb)\n",
    "  IVFPQindex.add(xb)\n",
    "  IVFPQindex.nprobe = nprobe \n",
    "\n",
    "  start = time.time()\n",
    "  IVFPQdist, IVFPQi = IVFPQindex.search(xq, k)\n",
    "  end = time.time()\n",
    "  IVFPQMem = lab.get_memory(IVFPQindex)\n",
    "  return [end-start, IVFPQMem, IVFPQdist, IVFPQi]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# data we will search through \n",
    "xb = lab.read_fvecs('./sift/sift_base.fvecs')  # 1M samples\n",
    "# also get some query vectors to search with\n",
    "xq = lab.read_fvecs('./sift/sift_query.fvecs')\n",
    "# take just a hundred query (there are many in sift_query)\n",
    "xq = xq[:100]\n",
    "print(xb.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Exhaustive Search NN\n",
    "# for each query vector in xq, it looks for the k=5 nearest neighbors in the xb. \n",
    "\n",
    "ESpred = ESNN(xq=xq, xb=xb, k=5)\n",
    "[EStime, ESmem, ESdist, ESi] = ESpred # Ndist: Euclidean distance, Ni: refers to the indices of the 5 NN.\n",
    "numQ = xq.shape[0]\n",
    "print('Exhaustive Search, searching {} vectors, time: {:.3f}s, memory: {:.2f}MB, recall: {:.2f}'.format(numQ, EStime, ESmem, 1.0))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "xb.shape[0] * 2*2/8/(1024*1024)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "xb.shape[0] * 2/(1024*1024)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "1.91 - 0.476837158203125"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "1.92 - 1.433162841796875"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Product Quantization NN\n",
    "# m: number of subvectors you'd like to split\n",
    "# nbits: number of bits per subquantizer\n",
    "\n",
    "for m in [2, 4, 8, 16]:\n",
    "  for nbits in [2, 4, 6, 8]:\n",
    "    PQpred = PQNN(xq=xq, xb=xb, k=5, m=m, nbits=nbits)\n",
    "    [PQtime, PQmem, PQdist, PQi] = PQpred\n",
    "    numQ = xq.shape[0]\n",
    "    recall = lab.compareRecall(gt=ESi, pred=PQi)\n",
    "    print('PQ NN, searching {} vectors, m: {}, nbits: {}, time: {:.3f}s, memory: {:.2f}MB, recall: {:.2f}'.format(numQ, m, nbits, PQtime, PQmem, recall))\n",
    "  print('-'*60)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "PQpred = PQNN(xq=xq, xb=xb, k=5, m=4, nbits=16)\n",
    "[PQtime, PQmem, PQdist, PQi] = PQpred\n",
    "numQ = xq.shape[0]\n",
    "recall = lab.compareRecall(gt=ESi, pred=PQi)\n",
    "print('PQ NN, searching {} vectors, m: {}, nbits: {}, time: {:.3f}s, memory: {:.2f}MB, recall: {:.2f}'.format(numQ, m, nbits, PQtime, PQmem, recall))"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Product Quantization NN\n",
    "# m: number of subvectors you'd like to split\n",
    "# nbits: number of bits per subquantizer\n",
    "\n",
    "for m in [2, 4, 8, 16]:\n",
    "  for nbits in [2, 4, 6, 8]:\n",
    "    PQpred = PQNN(xq=xq, xb=xb, k=5, m=m, nbits=nbits)\n",
    "    [PQtime, PQmem, PQdist, PQi] = PQpred\n",
    "    numQ = xq.shape[0]\n",
    "    recall = lab.compareRecall(gt=ESi, pred=PQi)\n",
    "    print('PQ NN, searching {} vectors, m: {}, nbits: {}, time: {:.3f}s, memory: {:.2f}MB, recall: {:.2f}'.format(numQ, m, nbits, PQtime, PQmem, recall))\n",
    "  print('-'*60)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Product Quantization NN\n",
    "# m: number of subvectors you'd like to split\n",
    "# nbits: number of bits per subquantizer\n",
    "\n",
    "for m in [2, 4, 8, 16]:\n",
    "  for nbits in [2, 4, 6, 8]:\n",
    "    PQpred = PQNN(xq=xq, xb=xb, k=5, m=m, nbits=nbits)\n",
    "    [PQtime, PQmem, PQdist, PQi] = PQpred\n",
    "    numQ = xq.shape[0]\n",
    "    recall = lab.compareRecall(gt=ESi, pred=PQi)\n",
    "    print('PQ NN, searching {} vectors, m: {}, nbits: {}, time: {:.3f}s, memory: {:.2f}MB, recall: {:.2f}'.format(numQ, m, nbits, PQtime, PQmem, recall))\n",
    "  print('-'*60)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Question: How can you further increase the recall ?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# IVFPQ NN\n",
    "# nlist:  # how many Voronoi cells\n",
    "# nbits: # when using IVF+PQ, higher nbits values are not supported\n",
    "# nprobe: # of cells you'd like to search.\n",
    "nbits = 8\n",
    "nlist = 2048\n",
    "nprobe = 2\n",
    "IVFPQpred = IVFPQNN(xq=xq, xb=xb, k=5, nlist=nlist, nbits=nbits, nprobe=nprobe)\n",
    "numQ = xq.shape[0]\n",
    "[IVFPQtime, IVFPQmem, IVFPQdist, IVFPQi] = IVFPQpred\n",
    "recall = lab.compareRecall(gt=ESi, pred=IVFPQi)\n",
    "print('IVFPQ NN, searching {} vectors, nlist: {}, nbits: {}, nprobe: {}, time: {:.3f}s, memory: {:.2f}MB, recall: {:.2f}'.format(numQ, nlist, nbits, nprobe, IVFPQtime, IVFPQmem, recall))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Question: try to increase nprobe, what happens ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More detailed benchmark can be found at http://ann-benchmarks.com/faiss-ivf.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
