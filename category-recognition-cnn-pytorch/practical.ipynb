{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG Image Classification Practical \n",
    "*By Andrea Vedaldi and Andrew Zisserman*\n",
    "\n",
    "This is an [Oxford Visual Geometry Group](http://www.robots.ox.ac.uk/~vgg) computer vision practical, authored by [Andrea Vedaldi](http://www.robots.ox.ac.uk/~vedaldi/) and Andrew Zisserman (Release 2018a).\n",
    "\n",
    "<img style=\"width:100%\" src=\"data/figures/cover.jpeg\" alt=\"cover\"/>\n",
    "\n",
    "This practical is on image classification, where an image is classified according to its visual content. For example, does it contain an airplane or not. Important applications are image retrieval - searching through an image dataset to obtain (or retrieve) those images with particular visual content, and image annotation - adding tags to images if they contain particular object categories.\n",
    "\n",
    "The goal of this session is to get basic practical experience with classification. It includes: (i) training a visual classifier for three different image classes (airplanes, motorbikes and people); (ii) assessing the performance of the classifier by computing a precision-recall curve; (iii) training set and testing set augmentation; and (iv) obtaining training data for new classifiers using Bing image search and using the classifiers to retrieve images from a dataset.\n",
    "\n",
    "$$\n",
    "\\newcommand{\\bx}{\\mathbf{x}}\n",
    "\\newcommand{\\by}{\\mathbf{y}}\n",
    "\\newcommand{\\bz}{\\mathbf{z}}\n",
    "\\newcommand{\\bw}{\\mathbf{w}}\n",
    "\\newcommand{\\bp}{\\mathbf{p}}\n",
    "\\newcommand{\\cP}{\\mathcal{P}}\n",
    "\\newcommand{\\cN}{\\mathcal{N}}\n",
    "\\newcommand{\\vv}{\\operatorname{vec}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi'] = 75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Training and testing an Image Classifier \n",
    "\n",
    "### Part 1.1: Data preparation\n",
    "\n",
    "The data provided in the directory data consists of images and pre-computed descriptors for each image. The JPEG images are contained in data/images. The data consists of three image classes (containing airplanes, motorbikes or persons) and \"background\" images (i.e. images that do not contain these three classes). In the data preparation stage, this data is divided as:\n",
    "\n",
    "| --     | aeroplane | motorbike | person | background |\n",
    "|--------|-----------|-----------|--------|------------|\n",
    "| train  | 112       | 120       | 1025   | 1019       |\n",
    "| test   | 126       | 125       | 983    | 1077       |\n",
    "| total  | 238       | 245       | 2008   | 2096       |\n",
    "\n",
    "The images are listed in a number of text files; for example, files [`data/motorbike_train.txt`](data/motorbike_train.txt) and [`data/motorbike_val.txt`](data/motorbike_val.txt) list images that contain motorbikes. However, there is no need to access such files directly. The provided `lab` module contains various utility functions useful in this practical, including `lab.get_image_database()` for loading a \"database\" structure `imdb` with an index of these images. The following code demonstrates using this structure and the function `lab.get_indices()` in order to reproduce the table above. Note, in the following, we will use the val set as the test set.\n",
    "\n",
    "> **Task:** Use the code below to load the image database structure and reproduce the table above.\n",
    "\n",
    "> **Remark:** The same image may contain more than one class. Such images are counted multiple times in the table above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lab\n",
    "lab.select_random_gpu()\n",
    "\n",
    "imdb = lab.get_image_database()\n",
    "\n",
    "total = 0\n",
    "for class_name in ['aeroplane', 'motorbike', 'person', 'background']:\n",
    "    for set_name in ['train', 'val']:\n",
    "        indices = lab.get_indices(imdb, class_name, set_name)\n",
    "        n = len(indices)\n",
    "        total += n\n",
    "        print(f\"Number of {class_name} images for {set_name}: {n}\")\n",
    "print(f\"Number of total examples: {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An image is represented by a single vector descriptor. Mapping the visual content of an image to a single descriptor vector is often regarded as an encoding step, and the resulting descriptor is sometimes called a code. The main benefit of working with fixed length vectors is that they can be compared by simple vectorial metrics such as Euclidean distance. For the same reason, they are a natural representation to use in learning an image classifier.\n",
    "\n",
    "We will use a *Convolutional Neural Network* (CNN) encoding. The process of constructing the CNN descriptor starting from an image is summarized next:\n",
    "\n",
    "<img style=\"width:100%\" src=\"data/figures/encoding.png\" alt=\"cover\"/>\n",
    "\n",
    "First, the network is pre-trained on the ImageNet dataset to classify an image into one of a thousand categories. This determines all the parameters of the CNN, such as the weights of the convolutional filters. Then, for a new image, the trained network is used to generate a descriptor vector from the response of a layer in the architecture with this image as input. For this practical we will use the AlexNet network, which produces a 4096 dimensional descriptor vector at the last fully-connected layer before classification, called *fc7*. We also consider computationally cheaper but weaker features extracted from the convolutional layers. In particular, we consider the following **encodings**:\n",
    "\n",
    "1. `conv3`: the third convolutional layer output, containing $2\\times 2 \\times 384$ feature channels.\n",
    "2. `conv4`: the fourth convolutional layer output, containing $2\\times 2 \\times 256$ feature channels.\n",
    "3. `conv5`: the fifth convolutional layer output, also containing $2\\times 2 \\times 256$ feature channels.\n",
    "4. `fc7`: the last fully-connected layer output, containing 4096 feature channels.\n",
    "\n",
    "The pre-computed codes can be computed by using the `lag.get_codes()` function.\n",
    "\n",
    "> **Task:** Use the code below to load the feature vectors in a tensor `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the code vectors (for AlexNet layer conv3)\n",
    "x = lab.get_codes(layer=3)\n",
    "\n",
    "print(f\"The tensor codes has size {list(x.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The tensor `x` has one image code vector per row (when the CNN output is a 3D tensor, the latter is vectorized). There are more than 4587 (total number of images) rows as `x` contains code vectors for additional images, as discussed below in the data augmentation section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2: Train a classifier for images containing motorbikes\n",
    "\n",
    "We will start by training a classifier for images that contain motorbikes. The motorbike training images will be used as the positives, and the background images as negatives. The classifier is a linear Support Vector Machine (SVM).\n",
    "\n",
    "> **Tasks:**\n",
    "> * Use the code below to display some images in the database.\n",
    "> * Look through example images of the motorbike class and the background images by changing the code as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some training images\n",
    "c = imdb['class_names'].index('motorbike')\n",
    "s = imdb['set_names'].index('train')\n",
    "indices = lab.get_indices(imdb, c, s)\n",
    "indices = indices[:16]\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "for t, index in enumerate(indices):\n",
    "    image = lab.get_image(imdb, index)\n",
    "    plt.gcf().add_subplot(4,4,t+1)\n",
    "    lab.imsc(image[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training an SVM, it is usually a good idea to normalize the data. Here we simply make all vectors have unit norm.\n",
    "\n",
    "> **Task:** Run the code below to normalize the code vectors in the tensor `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Normalize the code vectors\n",
    "x /= torch.sqrt((x * x).sum(1))[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can train the model, we must select an appropriate set of positive training and validation images. For that, we use the provided `lab.get_indices()` functions. The positive images are motorbike images in the training set. The negative images are aeroplane, person, and background images in the training set, minus any positive image (this is required as the same image can contain multiple classes).\n",
    "\n",
    "> **Task:** Run the code below to get the indices of the positive and negative training images. For now use only 20 positive training images, as indicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training images\n",
    "pos_train = lab.get_indices(imdb, 'motorbike',  'train')\n",
    "neg_train = lab.get_indices(imdb, ['aeroplane', 'person', 'background'], 'train', minus=pos_train)\n",
    "\n",
    "# Limit the training set size\n",
    "pos_train = pos_train[:20]\n",
    "neg_train = neg_train[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to run the support vector machine optimizer. For that, we use the *stochastic dual coordinate ascent (SDCA)* algorithm which produces an upper and lower bound on the classifier objective function and can thus detect convergence automatically.\n",
    "\n",
    "> **Task:** Run the code below to train the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(w, b, x):\n",
    "    return x @ w + b\n",
    "\n",
    "def train(x, pos, neg, lam=1e-4):\n",
    "    print(f\"Train an SVM using {len(pos)} positive, {len(neg)} negative examples, and regularization {lam}.\")\n",
    "\n",
    "    # Extract training codes and labels\n",
    "    x_train = x[torch.cat((pos, neg))]\n",
    "    c_train = torch.tensor([1] * len(pos) + [-1] * len(neg), dtype=torch.float32)\n",
    "\n",
    "    # Train the SVM using the SDCA solver\n",
    "    w, b = lab.svm_sdca(x_train, c_train, lam=lam)\n",
    "    \n",
    "    # Get the training data scores\n",
    "    scores_train = get_scores(w, b, x_train)    \n",
    "    return w, b, c_train, scores_train\n",
    "    \n",
    "w, b, c_train, scores_train = train(x, pos_train, neg_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first assess qualitatively how well the classifier works by using it to rank all the training images.\n",
    "\n",
    "> **Question:** What do you expect to happen? \n",
    "\n",
    "> **Task:** Use the code below to show the ranked list using the provided function `lab.plot_ranked_list()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort images by decreasing scores\n",
    "_, perm = torch.sort(scores_train, descending=True)\n",
    "\n",
    "# Show ranked \n",
    "plt.figure(1, figsize=(15,15))\n",
    "lab.plot_ranked_list(imdb, pos_train, neg_train, perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.3: Classify the test images and assess the performance\n",
    "\n",
    "Now apply the learnt classifier to the test images. Again, you can look at the qualitative performance by using the classifier score to rank all the test images. Note the bias term is not needed for this ranking, only the classification vector `w`.\n",
    "\n",
    "> **Question:** Why is the bias term not needed?\n",
    "\n",
    "> **Task:** Use the code below to evaluate the SVM on the validation data and plot the corresponding ranked list of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the validation images\n",
    "pos_val = lab.get_indices(imdb, 'motorbike',  'val')\n",
    "neg_val = lab.get_indices(imdb, ['aeroplane', 'person', 'background'], 'val', minus=pos_val)\n",
    "\n",
    "# Extract the validation codes\n",
    "x_val = x[torch.cat((pos_val, neg_val))]\n",
    "\n",
    "# Compute the validation scores\n",
    "scores_val = get_scores(w, b, x_val)\n",
    "\n",
    "# Sort images by decreasing scores\n",
    "_, perm_val = torch.sort(scores_val, descending=True)\n",
    "\n",
    "# Show ranked images\n",
    "plt.figure(1, figsize=(15,15))\n",
    "lab.plot_ranked_list(imdb, pos_val, neg_val, perm_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will measure the retrieval performance quantitatively by computing a Precision-Recall curve. Recall the definitions of **precision** and **recall**:\n",
    "\n",
    "<img style=\"height:15em\" src=\"data/figures/pr1.png\" alt=\"pr1\"/>\n",
    "\n",
    "The *precision-recall* (PR) curve is computed by varying the threshold on the classifier (from high to low) and plotting the values of precision against recall for each threshold value. In order to assess the retrieval performance by a single number (rather than a curve), the *Average Precision* (AP, the area under the curve) is often computed. Make sure you understand how the precision values in the Precision-Recall curve correspond to the ranking of the positives and negatives in the retrieved results.\n",
    "\n",
    "> **Task:** Run the code below to view the precision-recall curves using the provided function `lab.pr()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the validation labels\n",
    "c_val = torch.cat((torch.ones(len(pos_val)), -torch.ones(len(neg_val))))\n",
    "\n",
    "# Compute the precision-recall curves\n",
    "plt.figure(figsize=(6,6))\n",
    "_, _, ap_train = lab.pr(c_train, scores_train)\n",
    "_, _, ap_val = lab.pr(c_val, scores_val)\n",
    "plt.legend((f'train ({100*ap_train:.1f}%)', f'val ({100*ap_val:.1f}%)')) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.4: Increase the number of training images\n",
    "\n",
    "So far we have used only 20 (of the 120) positive images for training the classifier.\n",
    "\n",
    "> **Task:** Use the code below to retrain the SVM after changing the number of training images, particularly for the positive images, and measure the effect.\n",
    "\n",
    "> **Questions:** \n",
    "> \n",
    ">   * How much does the performance vary as `numPos` is changed from 1 to `+inf` (indicating that all available positive images are used)?\n",
    ">   * Compare doubling the number of training images by either including fresh samples or generating new samples using augmentation. Which performs better? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training images\n",
    "pos_train = lab.get_indices(imdb, 'motorbike',  'train')\n",
    "neg_train = lab.get_indices(imdb, ['aeroplane', 'person', 'background'], 'train', minus=pos_train)\n",
    "\n",
    "# Limit the training set size\n",
    "pos_train = pos_train[:]\n",
    "neg_train = neg_train[:]\n",
    "\n",
    "# Train the SVM\n",
    "w, b, _, _ = train(x, pos_train, neg_train)\n",
    "\n",
    "# Evalaute the SVM\n",
    "scores_val = get_scores(w, b, x_val)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "_, _, ap_val = lab.pr(c_val, scores_val)\n",
    "message = f'pos {len(pos_train)} neg {len(neg_train)} ap {100*ap_val:.1f}%'\n",
    "plt.legend((message,)) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Part 1.5: Increase the number of training images by data augmentation\n",
    "\n",
    "Up to this point the descriptor vector has been computed from the image as is. We now consider representing each image multiple times by generating a descriptor for both the original image and for the image after `flipping' (a mirror reflectance about a vertical axis). This data augmentation will be used in a different manner at training and test time. In training, the descriptors from the flipped images will be used as additional training samples (i.e. each original image generates two data samples for training). In testing, the descriptors for the original and flipped image will be averaged resulting in, again, a single vector representing the test image.\n",
    "\n",
    "> **Task:** Use the code below to retrain the SVM including the training and test augmentation and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and normalize the code vectors\n",
    "x = lab.get_codes(layer=3)\n",
    "x /= torch.sqrt((x * x).sum(1))[:,None]\n",
    "\n",
    "# Train with normal images together, as before\n",
    "pos_train = lab.get_indices(imdb, 'motorbike',  'train', jitters=['normal'])\n",
    "neg_train = lab.get_indices(imdb, ['aeroplane', 'person', 'background'], 'train',\n",
    "                            jitters=['normal'], minus=pos_train)\n",
    "w, b, _, _ = train(x, pos_train, neg_train)\n",
    "\n",
    "# Retrain with normal and jitter images together\n",
    "pos_train_aug = lab.get_indices(imdb, 'motorbike',  'train', jitters=['normal', 'flipped'])\n",
    "neg_train_aug = lab.get_indices(imdb, ['aeroplane', 'person', 'background'], 'train',\n",
    "                                jitters=['normal', 'flipped'], minus=pos_train_aug)\n",
    "w_aug, b_aug, _, _ = train(x, pos_train_aug, neg_train_aug)\n",
    "\n",
    "# Get the normal version of the validation images\n",
    "pos_val = lab.get_indices(imdb, 'motorbike',  'val')\n",
    "neg_val = lab.get_indices(imdb, ['aeroplane', 'person', 'background'], 'val', minus=pos_val)\n",
    "x_val = x[torch.cat((pos_val, neg_val))]\n",
    "\n",
    "# Get the jittered version of the validation images\n",
    "pos_val_jit = lab.get_indices(imdb, 'motorbike', 'val', jitters=['flipped'])\n",
    "neg_val_jit = lab.get_indices(imdb, ['aeroplane', 'person', 'background'], 'val',\n",
    "                              jitters=['flipped'], minus=pos_val_jit)\n",
    "x_val_jit = x[torch.cat((pos_val_jit, neg_val_jit))]\n",
    "\n",
    "# Evaluate the model trained without and with jitter using validation data without and with jitter\n",
    "scores_val               = get_scores(w, b, x_val)\n",
    "scores_train_aug_val     = get_scores(w_aug, b_aug, x_val)\n",
    "scores_val_aug           = get_scores(w, b, x_val_jit) + scores_val\n",
    "scores_train_aug_val_aug = get_scores(w_aug, b_aug, x_val_jit) + scores_train_aug_val\n",
    "\n",
    "# Compute the precision-recall curves\n",
    "plt.figure(figsize=(6,6))\n",
    "_, _, ap_val               = lab.pr(c_val, scores_val)\n",
    "_, _, ap_train_aug_val     = lab.pr(c_val, scores_train_aug_val)\n",
    "_, _, ap_val_aug           = lab.pr(c_val, scores_val_aug)\n",
    "_, _, ap_train_aug_val_aug = lab.pr(c_val, scores_train_aug_val_aug)\n",
    "\n",
    "plt.legend((f'no aug ({100*ap_val:.1f}%)', \n",
    "            f'train aug ({100*ap_train_aug_val:.1f}%)',\n",
    "            f'val aug ({100*ap_val_aug:.1f}%)',\n",
    "            f'train & val aug ({100*ap_train_aug_val_aug:.1f}%)',\n",
    "           )) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the change in classification performance if: (i) only training data augmentation is used, (ii) only testing data augmentation is used; and (iii) both training and test data are augmented.\n",
    "\n",
    "> **Questions:**\n",
    "> \n",
    "> * Is classifying the average vector for the test image the same as classifying each vector independently and then averaging the classifier score?\n",
    "> * When would you expect flipping augmentation to be detrimental to performance?\n",
    "> * How could additional descriptors be obtained from each image?\n",
    "\n",
    "**Note:** when learning the SVM, to save training time we are not changing the $\\lambda$ parameter controlling the regularization strength. This parameter influences the generalization error and should be relearnt on a validation set when the training setting is changed. However, in this case the influence of $\\lambda$ is small as can be verified experimentally (see the next [section](#stage1f))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=stage1f></a>\n",
    "### Part 1.6: Setting the hyper-parameter lambda of the SVM\n",
    "\n",
    "If there is a significant difference between the training and test performance, then that indicates over fitting. The difference can often be reduced, and the test performance (generalization), improved by changing the SVM $\\lambda$ parameter. \n",
    "\n",
    "> **Task:** Use the code below to vary the $\\lambda$ parameter in the range 1 to 1e-5 (the default is $\\lambda$=1e-4), and plot the AP on the training and test data as $\\lambda$ varies.\n",
    "\n",
    "> **Question:** How do validation performance and training time vary as $\\lambda$ is changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training images\n",
    "pos_train = lab.get_indices(imdb, 'motorbike',  'train')\n",
    "neg_train = lab.get_indices(imdb, ['aeroplane', 'person', 'background'], 'train', minus=pos_train)\n",
    "\n",
    "# Train the SVM\n",
    "w, b, _, _ = train(x, pos_train, neg_train, lam=0.001)\n",
    "\n",
    "# Evaluate the SVM\n",
    "scores_val = get_scores(w, b, x_val)\n",
    "plt.figure(figsize=(6,6))\n",
    "_, _, ap_val = lab.pr(c_val, scores_val)\n",
    "message = f'pos {len(pos_train)} neg {len(neg_train)} ap {100*ap_val:.1f}%'\n",
    "plt.legend((message,)) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** hyper-parameters and performance should actually be assessed on a validation set that is held out from the training set. They should not be assessed on the test set. In this practical we are not enforcing this good practice, but don't optimize on the test set once you move on from this practical and start to classify your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.7: Learn a classifier for the other categories and assess its performance\n",
    "\n",
    "*Skip [below](#stage1h) on fast track*\n",
    "\n",
    "> **Task:** Use the code below to train two object categories: aeroplane and people. In each case record the AP performance measure.\n",
    "\n",
    "> **Question:** Does the AP performance match your expectations based on the variation of the class images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training images and train the SVM\n",
    "pos_train = lab.get_indices(imdb, 'aeroplane',  'train')\n",
    "neg_train = lab.get_indices(imdb, ['motorbike', 'person', 'background'], 'train', minus=pos_train)\n",
    "w, b, c_train, scores_train = train(x, pos_train, neg_train)\n",
    "\n",
    "# Get the validation images and labels\n",
    "pos_val = lab.get_indices(imdb, 'aeroplane',  'val')\n",
    "neg_val = lab.get_indices(imdb, ['motorbike', 'person', 'background'], 'val', minus=pos_val)\n",
    "x_val = x[torch.cat((pos_val, neg_val))]\n",
    "c_val = torch.cat((torch.ones(len(pos_val)), -torch.ones(len(neg_val))))\n",
    "\n",
    "# Evaluate the SVM\n",
    "scores_val = get_scores(w, b, x_val)\n",
    "plt.figure(figsize=(6,6))\n",
    "_, _, ap_train = lab.pr(c_train, scores_train)\n",
    "_, _, ap_val = lab.pr(c_val, scores_val)\n",
    "plt.legend((f'train {100*ap_train:.1f}%', f'val {100*ap_val:.1f}%')) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=stage1h></a>\n",
    "### Part 1.8 :Vary the image representation\n",
    "\n",
    "An important practical aspect of image descriptors is their normalization. For example, if we regard the CNN descriptor as a discrete probability distribution it would seem natural that its elements should sum to 1. This is the same as normalizing the descriptor vectors in the L1 norm. However, above L2 normalization (sum of squares) is used instead.\n",
    "\n",
    "> **Task:** Use the following code to experiment with L1 normalization and no normalization and measure the performance change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 5\n",
    "num_pos = 20\n",
    "\n",
    "# Get the code vectors and normalize them\n",
    "x = lab.get_codes(layer=layer)\n",
    "#x /= torch.sqrt((x * x).sum(1))[:,None]\n",
    "#x /= torch.abs(x).sum(1)[:,None]\n",
    "\n",
    "# Get the training images and train the SVM\n",
    "pos_train = lab.get_indices(imdb, 'motorbike', 'train')\n",
    "neg_train = lab.get_indices(imdb, ['aeroplane', 'person', 'background'], 'train', minus=pos_train)\n",
    "pos_train = pos_train[:num_pos]\n",
    "w, b, c_train, scores_train = train(x, pos_train, neg_train)\n",
    "\n",
    "# Get the validation images and labels\n",
    "pos_val = lab.get_indices(imdb, 'motorbike', 'val')\n",
    "neg_val = lab.get_indices(imdb, ['aeroplane', 'person', 'background'], 'val', minus=pos_val)\n",
    "x_val = x[torch.cat((pos_val, neg_val))]\n",
    "c_val = torch.cat((torch.ones(len(pos_val)), -torch.ones(len(neg_val))))\n",
    "\n",
    "# Evaluate the SVM\n",
    "scores_val = get_scores(w, b, x_val)\n",
    "plt.figure(figsize=(6,6))\n",
    "_, _, ap_train = lab.pr(c_train, scores_train)\n",
    "_, _, ap_val = lab.pr(c_val, scores_val)\n",
    "plt.legend((f'train {100*ap_train:.1f}%', f'val {100*ap_val:.1f}%')) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear SVM can be thought of as using a linear kernel\n",
    "$$\n",
    " K(\\mathbf{h},\\mathbf{h}') = \\sum_{i=1}^d h_i h'_i\n",
    "$$\n",
    "to measure the similarity between pair of objects $h$ and $h'$ (in this case pairs of CNN descriptors).\n",
    "\n",
    "> **Question:** What can you say about the self-similarity,$K(\\mathbf{h},\\mathbf{h})$, of a descriptor $\\mathbf{h}$ that is L2 normalized?\n",
    "\n",
    "Compare $K(\\mathbf{h},\\mathbf{h})$ to the similarity, $K(\\mathbf{h},\\mathbf{h}')$,of two different L2 normalized descriptors $\\mathbf{h}$ and $\\mathbf{h}'$\n",
    "\n",
    "> **Questions:**\n",
    "> * Can you say the same for unnormalized or L1 normalized descriptors?\n",
    "> * Do you see a relation between the classification performance and L2 normalization?\n",
    "\n",
    "A useful rule of thumb is that better performance is obtained if the vectors that are ultimately fed to a linear SVM (after any intermediate processing) are L2 normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.9: Vary the CNN representation\n",
    "\n",
    "The CNN image features are very strong because they are pretrained on millions of images from the ImageNet data. So far, we have experimented with the `conv5` features. Try now to extract features from a different layer in the architecture and observe how the performance changes:\n",
    "\n",
    "> **Tasks**:\n",
    ">\n",
    "> * Restore L2 normalization for the image representation (see the previous stage) and choose a category.\n",
    "> * Rerun classification using only 10 training images by and pick code vectors from `conv1`, `conv2`, `conv3`, `conv4`, `conv5`, `fc6` and `fc7`.\n",
    "> * Note the resulting performance.\n",
    "\n",
    "> **Question:** How much does the choice of feature depth affect classification performance?\n",
    "\n",
    "Now make the setup even more extreme by considering the so-called one-shot learning problem, i.e. learning an image classifier from a single training image. Thus, set `pos_train` to one element and rerun the experiments.\n",
    "\n",
    "> **Questions:**\n",
    "> \n",
    "> * Can you get good performance using a single training images and the deeper features?\n",
    "> * If so, how is it possible that the system can learn to recognize an object from a single example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.10: Visualize class saliency\n",
    "\n",
    "\n",
    "You can use the function `lab.plot_saliency()` to visualize the areas of the image that the classifier thinks are most related to the class . Use this function first to visualize the class saliency using `fc7` features for an image containing the object category (adjust and rerun the cell above to train the corresponding model if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a positive validation image as a PIL image\n",
    "im_pil = lab.get_pil_image(imdb, pos_val[4])\n",
    "\n",
    "# Get the encoder neural network and normalize the image accordingly\n",
    "model = lab.get_encoder_cnn()\n",
    "im = model.normalize(im_pil)\n",
    "\n",
    "# Plot the saliency\n",
    "plt.figure(1, figsize=(12,12))\n",
    "lab.plot_saliency(model, layer, w, im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:** Do the areas correspond to the regions that you would expect to be selected?\n",
    "\n",
    "*Skip to [Part 2](#part2) on fast track*\n",
    "\n",
    "> **Tasks**:\n",
    ">\n",
    "> * Visualize the class saliency using features from other layers, `conv1` to `fc7`.\n",
    "> * Rerun the visualizations for an image not containing the object category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=part2></a>\n",
    "## Part 2: Training an Image Classifier for Retrieval using Internet images\n",
    "\n",
    "In Part 1 of this practical the training data was provided and all the feature vectors pre-computed. The goal of this second part is to choose the training data yourself in order to optimize the classifier performance. The task is the following: you are given a large corpus of images and asked to retrieve images of a certain class, e.g. those containing a bicycle. You then need to obtain training images, e.g. using Google Image Search, in order to train a classifier for images containing bicycles and optimize its retrieval performance.\n",
    "\n",
    "The code below provides the following functionality: it loads a number of image URLs from the internet and computes their code vectors. It then uses negative examples from the image database used above to train a classifier and rank the test images. To get started, we will train a classifier for *horses*.\n",
    "\n",
    "> **Task:** Modify the code below to load a certain number of images of horses from the internet. Google Image Search, Bing, or Wikimedia Commons are all good sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some images from the Internet and compute their code vectors\n",
    "cache = lab.ImageCache()\n",
    "cache.add('https://upload.wikimedia.org/wikipedia/commons/a/ab/Rock%27s-anne-supreme-winner.jpg')\n",
    "cache.add('https://upload.wikimedia.org/wikipedia/commons/thumb/2/25/Aegidienberger.jpg/528px-Aegidienberger.jpg')\n",
    "cache.add('https://upload.wikimedia.org/wikipedia/commons/thumb/9/9e/New_Moon_Girl_%28Little_Girl%29_Spanish_Mustang_Mare.jpg/640px-New_Moon_Girl_%28Little_Girl%29_Spanish_Mustang_Mare.jpg')\n",
    "cache.add('https://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Draft_horse.jpg/640px-Draft_horse.jpg')\n",
    "cache.add('https://upload.wikimedia.org/wikipedia/commons/thumb/9/9d/Quarter_Horse%28REFON%29-cleaned.jpg/640px-Quarter_Horse%28REFON%29-cleaned.jpg')\n",
    "\n",
    "plt.figure(1, figsize=(12,8))\n",
    "cache.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the positive code vectors for the images just downloaded are added to the negative code vectors from the image database and an SVM is trained.\n",
    "\n",
    "> **Task:** Run the code below to train the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data\n",
    "layer = 5\n",
    "\n",
    "# Get the code vectors and normalize them\n",
    "x_new = cache.get_codes(layer=layer)\n",
    "x_new /= torch.sqrt((x_new * x_new).sum(1))[:,None]\n",
    "\n",
    "x = lab.get_codes(layer=layer)\n",
    "x /= torch.sqrt((x * x).sum(1))[:,None]\n",
    "\n",
    "# Assemble training set using database images as negative examples\n",
    "neg_train = lab.get_indices(imdb, ['aeroplane', 'person', 'background'], 'train', minus=pos_train)\n",
    "x_train = torch.cat((x_new, x[neg_train]),0)\n",
    "c_train = torch.tensor([1] * len(cache) + [-1] * len(neg_train), dtype=torch.float32)\n",
    "\n",
    "# Train the SVM using the SDCA solver\n",
    "w, b = lab.svm_sdca(x_train, c_train, lam=1e-4)\n",
    "\n",
    "# Get the training data scores\n",
    "scores_train = get_scores(w, b, x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the classifier is trained we can assess it on a validation set. Fortunately `imdb` contains images for 148 validation images for the class *horse*, so we can use it as above. Your goal is to train a classifier that can retrieve as many of these as possible in a high ranked position. You can measure your success by looking at the average precision of the classifier on the validation set, as before.\n",
    "\n",
    "> **Task:** Run the code below to assess your newly-trained SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the validation images and labels\n",
    "pos_val = lab.get_indices(imdb, 'horse', 'val')\n",
    "neg_val = lab.get_indices(imdb, ['aeroplane', 'motorbike', 'person', 'background'], 'val', minus=pos_val)\n",
    "x_val = x[torch.cat((pos_val, neg_val))]\n",
    "c_val = torch.cat((torch.ones(len(pos_val)), -torch.ones(len(neg_val))))\n",
    "\n",
    "# Evaluate the SVM on the validation data\n",
    "scores_val = get_scores(w, b, x_val)\n",
    "\n",
    "# Precision-recall curves\n",
    "plt.figure(1, figsize=(6,6))\n",
    "_, _, ap_train = lab.pr(c_train, scores_train)\n",
    "_, _, ap_val = lab.pr(c_val, scores_val)\n",
    "plt.legend((f'train {100*ap_train:.1f}%', f'val {100*ap_val:.1f}%')) ;\n",
    "\n",
    "# Ranked image list\n",
    "_, perm = torch.sort(scores_val, descending=True)\n",
    "plt.figure(2, figsize=(15,15))\n",
    "\n",
    "lab.plot_ranked_list(imdb, pos_val, neg_val, perm) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some ways to improve the classifier:\n",
    "\n",
    "> **Tasks:**\n",
    "> * Add more positive training images.\n",
    "> * Add more positive training images, but choose these to be varied from those you already have.\n",
    "> * The validation data also contains the category car. Train classifiers for it and compare the difficulty of this and the horse class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links and further work\n",
    "\n",
    "* The code for this practical is written using the PyTorch library, which is freely available as source code and binary.\n",
    "* The images for this practical are taken from the [PASCAL VOC 2007 benchmark](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/).\n",
    "* For a tutorial on large scale image classification and references to the literature, see [here](https://sites.google.com/site/lsvr13/).\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "* Funding from ERC grant \"Integrated and Detailed Image Understanding\", and the EPSRC Programme Grant \"SeeBiByte\".\n",
    "\n",
    "<img style=\"height:100px\" src=\"data/figures/erc.jpg\" alt=\"erc\" />\n",
    "<img style=\"height:50px\" src=\"data/figures/epsrc.png\" alt=\"epsrc\" />\n",
    "\n",
    "## History\n",
    "\n",
    "* Used in the Oxford AIMS CDT, 2018-19 (PyTorch).\n",
    "* Used in the Oxford AIMS CDT, 2017-18.\n",
    "* Used in the Oxford AIMS CDT, 2016-17.\n",
    "* First used in the Oxford AIMS CDT, 2015-16.\n",
    "* Replaces the Image Categorization practical based on hand-crafted features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
