{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fs8RElVppdQX"
   },
   "source": [
    "# Vision Transformers\n",
    "\n",
    "Practical by Niki Amini-Naieni, Iro Laina, and Andrew Zisserman adapted and extended from tutorial by Phillip Lippe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-BTSEdD6Nir"
   },
   "source": [
    "This practical introduces Vision Transformers (ViTs) and explores their role in modern computer vision. Since [Alexey Dosovitskiy et al.](https://openreview.net/pdf?id=YicbFdNTTy) first demonstrated that Transformers could be applied successfully to image recognition tasks, the field has seen rapid developments. But how do these models work in practice, and how do they compare to more traditional convolutional networks? And how can we take advantage of large-scale pretraining to adapt state-of-the-art Transformer models to new tasks?\n",
    "\n",
    "You will explore these questions in two stages.\n",
    "\n",
    "In **Part 1**, you will implement and train a Vision Transformer from scratch on the CIFAR-10 dataset and compare its classification performance to a CNN trained under similar conditions. This will give practical experience with both architectures.\n",
    "\n",
    "In **Part 2**, you will investigate how Vision Transformers can be used effectively without training them end-to-end. You will extract frozen features from a model pretrained with self-supervision on large-scale datasets and train a linear classifier on top, and you will also evaluate CLIP’s zero-shot classification capabilities. This part shows how powerful pretrained representations can be when applied to smaller datasets, and how modern vision systems often rely on large-scale pretraining.\n",
    "\n",
    "If you are not familiar with Transformers yet, take a look at [this online tutorial](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html) where the fundamentals of Multi-Head Attention and Transformers are discussed. We will use [PyTorch Lightning](https://www.pytorchlightning.ai/) \\(introduced [here](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial5/Inception_ResNet_DenseNet.html)\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0SNdOlfNMj9"
   },
   "source": [
    "**IMPORTANT**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Please make sure to change your runtime type to GPU **before** starting. Click the dropdown menu in the top right and select \"Change Runtime Type.\" Then, check the circle next to any of the available GPUs.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2DkGRS2b4y-"
   },
   "source": [
    "# **Part 1**\n",
    "---\n",
    "## Learning Outcomes\n",
    "By the end of Part 1, you should be able to:\n",
    "\n",
    "* Load and use datasets and dataloaders in PyTorch\n",
    "\n",
    "* Reason about tensor shapes and dimensions\n",
    "\n",
    "* Understand and implement the core components of a Vision Transformer in code\n",
    "\n",
    "* Explain the memory requirements and scaling behavior of self-attention\n",
    "\n",
    "* Compare pre-layer and post-layer normalization and discuss their benefits and drawbacks\n",
    "\n",
    "* Understand the purpose of positional embeddings, explore different implementations, and evaluate their strengths and limitations\n",
    "\n",
    "* Explain the role of the CLS token\n",
    "\n",
    "* Interpret learning curves and compare the training dynamics of different architectures\n",
    "\n",
    "* Compare the behavior of CNNs and Vision Transformers when trained from scratch on the same small dataset, and explain the differences you observe\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wa7sKiYEpdQc"
   },
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "## Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10, Flowers102\n",
    "from torchvision import transforms\n",
    "\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install --quiet pytorch-lightning>=1.4\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Import tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial15\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GrFUmX1pdQe"
   },
   "source": [
    "We provide a pre-trained Vision Transformer which we download in the next cell. However, Vision Transformers can be relatively quickly trained on CIFAR10 with an overall training time of less than an hour on an NVIDIA TitanRTX. Feel free to experiment with training your own Transformer once you have gone through the whole notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOTt0U-KpdQe"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "# Github URL where saved models are stored for this tutorial\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/\"\n",
    "# Files to download\n",
    "pretrained_files = [\"tutorial15/ViT.ckpt\", \"tutorial15/tensorboards/ViT/events.out.tfevents.ViT\",\n",
    "                    \"tutorial5/tensorboards/ResNet/events.out.tfevents.resnet\"]\n",
    "# Create checkpoint path if it doesn't exist yet\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for file_name in pretrained_files:\n",
    "    file_path = os.path.join(CHECKPOINT_PATH, file_name.split(\"/\",1)[1])\n",
    "    if \"/\" in file_name.split(\"/\",1)[1]:\n",
    "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
    "    if not os.path.isfile(file_path):\n",
    "        file_url = base_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "        except HTTPError as e:\n",
    "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnTCxgTTpdQf"
   },
   "source": [
    "We load the CIFAR10 dataset below. We use the same setup of the datasets and data augmentations that were used for the CNNs we will benchmark against to keep a fair comparison. The constants in the `transforms.Normalize` correspond to the values that scale and shift the data to a zero mean and standard deviation of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smjKDmC2pdQf"
   },
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n",
    "                                     ])\n",
    "# For training, we add some augmentation. Networks have too much capacity and would overfit.\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n",
    "                                     ])\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "# We need to do a little trick because the validation set should not use the augmentation.\n",
    "train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=train_transform, download=True)\n",
    "val_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=test_transform, download=True)\n",
    "pl.seed_everything(42)\n",
    "train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
    "pl.seed_everything(42)\n",
    "_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000])\n",
    "\n",
    "# Loading the test set\n",
    "test_set = CIFAR10(root=DATASET_PATH, train=False, transform=test_transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "val_loader = data.DataLoader(val_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n",
    "test_loader = data.DataLoader(test_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "# Visualize some examples\n",
    "NUM_IMAGES = 4\n",
    "CIFAR_images = torch.stack([val_set[idx][0] for idx in range(NUM_IMAGES)], dim=0)\n",
    "img_grid = torchvision.utils.make_grid(CIFAR_images, nrow=4, normalize=True, pad_value=0.9)\n",
    "img_grid = img_grid.permute(1, 2, 0)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.title(\"Image examples of the CIFAR10 dataset\")\n",
    "plt.imshow(img_grid)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2v1rdVySt48u"
   },
   "source": [
    "# **Question 1**\n",
    "> Write code to print out the image size. What is the height and width of the images in CIFAR10?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UWJefukpdQf"
   },
   "source": [
    "## Transformers for image classification\n",
    "\n",
    "Transformers have been originally proposed to process sets since they are permutation-equivariant architectures, i.e., producing the same output permuted if the input is permuted. To apply Transformers to sequences, we have simply added a positional encoding to the input feature vectors, and the model learned by itself what to do with it. So, why not apply the same approach to images? This is exactly what [Alexey Dosovitskiy et al.](https://openreview.net/pdf?id=YicbFdNTTy) proposed in their paper \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\". Specifically, the Vision Transformer is a model, first used for image classification, that views images as sequences of smaller patches. As a preprocessing step, we split an image of, for example, $48\\times 48$ pixels into 9 $16\\times 16$ patches. Each of those patches is considered to be a \"word\"/\"token\" and projected to a feature space. With adding positional encodings and a token for classification (the \"class token\") on top, we can apply a Transformer as usual to this sequence and start training it for our task. A nice GIF visualization of the architecture is shown below (figure credit - [Phil Wang](https://github.com/lucidrains/vit-pytorch/blob/main/images/vit.gif)):\n",
    "\n",
    "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial15/vit.gif?raw=1\" width=\"600px\"></center>\n",
    "\n",
    "We will walk step by step through the Vision Transformer, and implement all parts by ourselves. First, let's implement the image preprocessing: an image of size $N\\times N$ has to be split into $(N/M)^2$ patches of size $M\\times M$. These represent the input words to the Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eOdV9_aZpdQg"
   },
   "outputs": [],
   "source": [
    "def img_to_patch(x, patch_size, flatten_channels=True):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        x - torch.Tensor representing the image of shape [B, C, H, W]\n",
    "        patch_size - Number of pixels per dimension of the patches (integer)\n",
    "        flatten_channels - If True, the patches will be returned in a flattened format\n",
    "                           as a feature vector instead of a image grid.\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n",
    "    x = x.permute(0, 2, 4, 1, 3, 5) # [B, H', W', C, p_H, p_W]\n",
    "    x = x.flatten(1,2)              # [B, H'*W', C, p_H, p_W]\n",
    "    if flatten_channels:\n",
    "        x = x.flatten(2,4)          # [B, H'*W', C*p_H*p_W]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2iy5jQMpdQg"
   },
   "source": [
    "Let's take a look at how that works for our CIFAR examples above. We choose a patch size of 4 and visualize the patches below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGKM3URHpdQg"
   },
   "outputs": [],
   "source": [
    "img_patches = img_to_patch(CIFAR_images, patch_size=4, flatten_channels=False)\n",
    "\n",
    "fig, ax = plt.subplots(CIFAR_images.shape[0], 1, figsize=(14,3))\n",
    "fig.suptitle(\"Images as input sequences of patches\")\n",
    "for i in range(CIFAR_images.shape[0]):\n",
    "    img_grid = torchvision.utils.make_grid(img_patches[i], nrow=64, normalize=True, pad_value=0.9)\n",
    "    img_grid = img_grid.permute(1, 2, 0)\n",
    "    ax[i].imshow(img_grid)\n",
    "    ax[i].axis('off')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOTm9iIQyXT0"
   },
   "source": [
    "# **Question 2**\n",
    "> Given the image size in Question 1, how many patches would we obtain per image using patch sizes 2, 4, 8, 16, and 32? In the above code, we set the patch size to 4. Add code below to print the shape of ```img_patches```. What does each dimension of ```img_patches``` represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeLThSYQ7hV1"
   },
   "source": [
    "# **Question 3**\n",
    "> How does increasing the number of patches ($n$) influence the required memory for attention and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtI9MzGbpdQg"
   },
   "source": [
    "Compared to the original images, it is much harder to recognize the objects from those patch lists now. Still, this is the input we provide to the Transformer for classifying the images. The model has to learn itself how it has to combine the patches to recognize the objects. The inductive bias in CNNs that an image is a grid of pixels, is lost in this input format.\n",
    "\n",
    "After we have looked at the preprocessing, we can now start building the Transformer model. The fundamentals of Multi-Head Attention are revisited [here](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html). For now, we will simply use the PyTorch module `nn.MultiheadAttention` ([docs](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html?highlight=multihead#torch.nn.MultiheadAttention)). Further, we use the Pre-Layer Normalization version of the Transformer blocks proposed by [Ruibin Xiong et al.](http://proceedings.mlr.press/v119/xiong20b/xiong20b.pdf) in 2020. The idea is to apply Layer Normalization not in between residual blocks, but instead as a first layer in the residual blocks. This reorganization of the layers supports better gradient flow and removes the necessity of a warm-up stage. A visualization of the difference between the standard Post-LN and the Pre-LN version is shown below.\n",
    "\n",
    "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial15/pre_layer_norm.svg?raw=1\" width=\"400px\"></center>\n",
    "\n",
    "The implementation of the Pre-LN attention block looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYWzrjtapdQg"
   },
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of input and attention feature vectors\n",
    "            hidden_dim - Dimensionality of hidden layer in feed-forward network\n",
    "                         (usually 2-4x larger than embed_dim)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            dropout - Amount of dropout to apply in the feed-forward network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads,\n",
    "                                          dropout=dropout)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp_x = self.layer_norm_1(x)\n",
    "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
    "        x = x + self.linear(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "828JLwpkAn6K"
   },
   "source": [
    "# **Question 4**\n",
    "> Why does Pre-Layer Normalization help with gradient flow and remove the need for a warm-up stage? Hint: review Theorem 1 in [Ruibin Xiong et al.](http://proceedings.mlr.press/v119/xiong20b/xiong20b.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyPv2JTipdQh"
   },
   "source": [
    "Now we have all modules ready to build our own Vision Transformer. Besides the Transformer encoder, we need the following modules:\n",
    "\n",
    "* A **linear projection** layer that maps the input patches to a feature vector of larger size. It is implemented by a simple linear layer that takes each $M\\times M$ patch independently as input.\n",
    "* A **classification token** that is added to the input sequence. We will use the output feature vector of the classification token (CLS token in short) for determining the classification prediction.\n",
    "* Learnable **positional encodings** that are added to the tokens before being processed by the Transformer. Those are needed to learn position-dependent information, and convert the set to a sequence. Since we usually work with a fixed resolution, we can learn the positional encodings instead of having the pattern of sine and cosine functions.\n",
    "* An **MLP head** that takes the output feature vector of the CLS token, and maps it to a classification prediction. This is usually implemented by a small feed-forward network or even a single linear layer.\n",
    "\n",
    "With those components in mind, let's implement the full Vision Transformer below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUJn0bCAGXi_"
   },
   "source": [
    "# **Question 5**\n",
    "> Here we opt to *learn* the positional embeddings instead of keeping them fixed. Why is this okay here? What could we do to handle higher resolution images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-hDilXspdQh"
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, num_channels, num_heads, num_layers, num_classes, patch_size, num_patches, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of the input feature vectors to the Transformer\n",
    "            hidden_dim - Dimensionality of the hidden layer in the feed-forward networks\n",
    "                         within the Transformer\n",
    "            num_channels - Number of channels of the input (3 for RGB)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            num_layers - Number of layers to use in the Transformer\n",
    "            num_classes - Number of classes to predict\n",
    "            patch_size - Number of pixels that the patches have per dimension\n",
    "            num_patches - Maximum number of patches an image can have\n",
    "            dropout - Amount of dropout to apply in the feed-forward network and\n",
    "                      on the input encoding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # Layers/Networks\n",
    "        self.input_layer = nn.Linear(num_channels*(patch_size**2), embed_dim)\n",
    "        self.transformer = nn.Sequential(*[AttentionBlock(embed_dim, hidden_dim, num_heads, dropout=dropout) for _ in range(num_layers)])\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Parameters/Embeddings\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,embed_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1,1+num_patches,embed_dim))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Preprocess input\n",
    "        x = img_to_patch(x, self.patch_size)\n",
    "        B, T, _ = x.shape\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        # Add CLS token and positional encoding\n",
    "        cls_token = self.cls_token.repeat(B, 1, 1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x = x + self.pos_embedding[:,:T+1]\n",
    "\n",
    "        # Apply Transforrmer\n",
    "        x = self.dropout(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Perform classification prediction\n",
    "        cls = x[0]\n",
    "        out = self.mlp_head(cls)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzXpzfGkO5VO"
   },
   "source": [
    "# **Question 6**\n",
    "> The code to initialize the input layer is: ```self.input_layer = nn.Linear(num_channels*(patch_size**2), embed_dim)```. What is the input layer doing in the forward function? Describe its role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6NUrktWRPOQ"
   },
   "source": [
    "# **Question 7**\n",
    "> Look at the forward function in the ```VisionTransformer``` class. At what position is the\"class\" token? What is its role?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cz_YeV6QpdQh"
   },
   "source": [
    "Finally, we can put everything into a PyTorch Lightning Module as usual. We use `torch.optim.AdamW` as the optimizer, which is Adam with a corrected weight decay implementation. Since we use the Pre-LN Transformer version, we do not need to use a learning rate warmup stage anymore. Instead, we use the same learning rate scheduler as the CNNs we will benchmark against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4KJAmXTQpdQh"
   },
   "outputs": [],
   "source": [
    "class ViT(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model_kwargs, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = VisionTransformer(**model_kwargs)\n",
    "        self.example_input_array = next(iter(train_loader))[0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,150], gamma=0.1)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "        self.log(f'{mode}_loss', loss)\n",
    "        self.log(f'{mode}_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._calculate_loss(batch, mode=\"train\")\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode=\"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCem5lOvpdQi"
   },
   "source": [
    "## Experiments\n",
    "\n",
    "Commonly, Vision Transformers are applied to large-scale image classification benchmarks such as ImageNet to leverage their full potential. However, here we take a step back and ask: can Vision Transformer also succeed on classical, small benchmarks such as CIFAR10? To find this out, we train a Vision Transformer from scratch on the CIFAR10 dataset. Let's first create a training function for our PyTorch Lightning module which also loads the pre-trained model if you have downloaded it above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57iCe30KpdQi"
   },
   "outputs": [],
   "source": [
    "def train_model(train, **kwargs):\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"ViT\"),\n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=180,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
    "                                    LearningRateMonitor(\"epoch\")])\n",
    "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"ViT.ckpt\")\n",
    "    if not train:\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        model = ViT.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n",
    "    else:\n",
    "        pl.seed_everything(42) # To be reproducable\n",
    "        model = ViT(**kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model = ViT.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_loader, verbose=False)\n",
    "    result = {\"test\": test_result[0][\"test_acc\"], \"val\": val_result[0][\"test_acc\"]}\n",
    "\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ReNJt5kpdQi"
   },
   "source": [
    "The next step would be to start training our model. As seen in our implementation, we have a couple of hyperparameters that we have to set. When creating this notebook, we have performed a small grid search over hyperparameters and listed the best hyperparameters in the cell below. Nevertheless, it is worth discussing the influence that each hyperparameter has, and what intuition we have for choosing its value.\n",
    "\n",
    "First, let's consider the patch size. The smaller we make the patches, the longer the input sequences to the Transformer become. While in general, this allows the Transformer to model more complex functions, it requires a longer computation time due to the way attention weights scale with sequence length, as discussed in Question 3. Furthermore, small patches can make the task more difficult since the Transformer has to learn which patches are close-by, and which are far away. We experimented with patch sizes of 2, 4, and 8 which gives us the input sequence lengths of 256, 64, and 16 respectively. We found 4 to result in the best performance and hence pick it below.\n",
    "\n",
    "Next, the embedding and hidden dimensionality have a similar impact on a Transformer as to an MLP. The larger the sizes, the more complex the model becomes, and the longer it takes to train. In Transformers, however, we have one more aspect to consider: the query-key sizes in the Multi-Head Attention layers. Each key has the feature dimensionality of `embed_dim/num_heads`. Considering that we have an input sequence length of 64, a minimum reasonable size for the key vectors is 16 or 32. Lower dimensionalities can restrain the possible attention maps too much. We observed that more than 8 heads are not necessary for the Transformer, and therefore pick an embedding dimensionality of `256`. The hidden dimensionality in the feed-forward networks is usually 2-4x larger than the embedding dimensionality, and thus we pick `512`.\n",
    "\n",
    "Finally, the learning rate for Transformers is usually relatively small, and in papers, a common value to use is 3e-5. However, since we work with a smaller dataset and have a potentially easier task, we found that we are able to increase the learning rate to 3e-4 without any problems. To reduce overfitting, we use a dropout value of 0.2. Remember that we also use small image augmentations as regularization during training.\n",
    "\n",
    "Feel free to explore the hyperparameters yourself by changing the values below. In general, the Vision Transformer did not show to be too sensitive to the hyperparameter choices on the CIFAR10 dataset.\n",
    "\n",
    "Instead of training the model, in the interest of time, we will just load the pretrained checkpoint. If you have the time, you can change ```train=False``` to ```train=True``` in the input to the ```train_model``` function and try retraining yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gn-KL_ZZpdQi"
   },
   "outputs": [],
   "source": [
    "model, results = train_model(train=False, model_kwargs={\n",
    "                                'embed_dim': 256,\n",
    "                                'hidden_dim': 512,\n",
    "                                'num_heads': 8,\n",
    "                                'num_layers': 6,\n",
    "                                'patch_size': 4,\n",
    "                                'num_channels': 3,\n",
    "                                'num_patches': 64,\n",
    "                                'num_classes': 10,\n",
    "                                'dropout': 0.2\n",
    "                            },\n",
    "                            lr=3e-4)\n",
    "print(\"ViT results\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VdwjUu2pdQi"
   },
   "source": [
    "The Vision Transformer achieves a validation and test performance of about 77%. In comparison, almost all CNN architectures tested [here](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial5/Inception_ResNet_DenseNet.html) obtain a classification performance of around 90%. This is a considerable gap and shows that although Vision Transformers perform strongly on ImageNet with potential pretraining, they cannot come close to simple CNNs on CIFAR10 when being trained from scratch. The differences between a CNN and Transformer can be well observed in the training curves. Let's look at them in a tensorboard below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyXnC7NWpdQi"
   },
   "outputs": [],
   "source": [
    "# Opens tensorboard in notebook. Adjust the path to your CHECKPOINT_PATH!\n",
    "%tensorboard --logdir ../saved_models/tutorial15/tensorboards/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "he_i9Y8mpdQi"
   },
   "source": [
    "<center><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial15/tensorboard_screenshot.png?raw=1\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXC2gMI4pdQi"
   },
   "source": [
    "The tensorboard compares the Vision Transformer to a ResNet trained on CIFAR10. When looking at the training losses, we see that the ResNet learns much more quickly in the first iterations. While the learning rate might have an influence on the initial learning speed, we see the same trend in the validation accuracy. The ResNet achieves the best performance of the Vision Transformer after just 5 epochs (2000 iterations). Further, while the ResNet training loss and validation accuracy have a similar trend, the validation performance of the Vision Transformers only marginally changes after 10k iterations while the training loss has almost just started going down. Yet, the Vision Transformer is also able to achieve close to 100% accuracy on the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDRJ1DdyA6oi"
   },
   "source": [
    "# **Question 8**\n",
    "> Based on these curves, do you think we could get improved performance by training for longer (i.e., for more epochs)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaaRU7JgbLqi"
   },
   "source": [
    "# **Question 9**\n",
    "> What would explain this behavior in the learning curves? Why is the vision transformer not able to achieve the same validation accuracy as the ResNet when trained on a small dataset like CIFAR10? What could we try to fix this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8DWp6TQmuGB"
   },
   "source": [
    "#✅**Checkpoint 1**\n",
    "Check your answers to Part 1 with a TA before moving on to Part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPyP2Jqccmwu"
   },
   "source": [
    "# **Part 2**\n",
    "---\n",
    "## Learning Outcomes\n",
    "As we have seen in Part 1, vision transformers underperform when trained from scratch on small datasets. Then how can we benefit from the vision transformer architecture when we do not have access to millions (or even billions) of annotated images for our task? We will answer this question in Part 2.\n",
    "\n",
    "By the end of Part 2, you should be able to:\n",
    "\n",
    "* Load and use pretrained Vision Transformer models from the transformers library\n",
    "\n",
    "* Visualize and interpret attention maps at different layers of a ViT\n",
    "\n",
    "* Adapt pretrained vision transformers to new classification tasks using linear probing\n",
    "\n",
    "* Perform and analyse zero-shot classification using CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsIKkfxEZ33Q"
   },
   "source": [
    "The first method we will investigate is adapting pretrained vision transformers to classification tasks using linear probing. Specifically, we will adapt [Dinov2](https://arxiv.org/abs/2304.07193), a vision transformer trained with self-supervision on millions of images, to flower classification. Start by loading the model from the transformers library in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTABQKbrkrrr"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoImageProcessor\n",
    "\n",
    "dinov2_vits14 = AutoModel.from_pretrained(\"facebook/dinov2-small\", trust_remote_code=True).to(device)\n",
    "dinov2_vits14.set_attn_implementation('eager')\n",
    "preprocess = AutoImageProcessor.from_pretrained(\"facebook/dinov2-small\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMBt8iC8b9Uv"
   },
   "source": [
    "You can visualize the model architecture and image pre-processing pipeline by printing them out in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FoAVyCjRmcp9"
   },
   "outputs": [],
   "source": [
    "print(dinov2_vits14)\n",
    "print(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lz0jsM84clC5"
   },
   "source": [
    "# **Question 10**\n",
    "> The model outputs pretrained features, vectors representing the image. Based on the output of the previous cell, how long is one of these vectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-KULzvcdED4"
   },
   "source": [
    "It is very important to remember that the vision transformer loaded in this part was *pretrained* on large-scale datasets. This pretraining enables it to operate as a good *feature extractor*, producing meaningful representations of images that can be used for downstream tasks such as classification.\n",
    "\n",
    "We can examine how meaningful the representations of the pretrained vision transformer are by visualising the attention maps at the final layer of the model. We will do this next for an example image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yP0gg_mCNgU"
   },
   "outputs": [],
   "source": [
    "def visualize_attention(attentions, img, patch_size, device):\n",
    "    # make the image divisible by the patch size\n",
    "    w, h = img.shape[1] - img.shape[1] % patch_size, img.shape[2] - \\\n",
    "        img.shape[2] % patch_size\n",
    "    img = img[:, :w, :h].unsqueeze(0)\n",
    "\n",
    "    w_featmap = img.shape[-2] // patch_size\n",
    "    h_featmap = img.shape[-1] // patch_size\n",
    "\n",
    "    nh = attentions.shape[1]  # number of head\n",
    "\n",
    "    # keep only the output patch attention\n",
    "    attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
    "\n",
    "    attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
    "    attentions = nn.functional.interpolate(attentions.unsqueeze(\n",
    "        0), scale_factor=patch_size, mode=\"bicubic\")[0].cpu().numpy()\n",
    "\n",
    "    return attentions\n",
    "\n",
    "\n",
    "def plot_attention(img, attention):\n",
    "    n_heads = attention.shape[0]\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    text = [\"Original Image\", \"Head Mean\"]\n",
    "    for i, fig in enumerate([img, np.mean(attention, 0)]):\n",
    "        plt.subplot(1, 2, i+1)\n",
    "        plt.imshow(fig, cmap='inferno')\n",
    "        plt.title(text[i])\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(n_heads):\n",
    "        plt.subplot(n_heads//3, 3, i+1)\n",
    "        plt.imshow(attention[i], cmap='inferno')\n",
    "        plt.title(f\"Head n: {i+1}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hT7fJdP7fSP3"
   },
   "source": [
    "Obtain the example image by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2wePxtbOCnkD"
   },
   "outputs": [],
   "source": [
    "! wget \"https://github.com/aryan-jadon/Medium-Articles-Notebooks/raw/main/Visualizing%20Attention%20in%20Vision%20Transformer/corgi_image.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geG2IEzXfknt"
   },
   "source": [
    "Visualise the attention maps at the final layer of the pretrained model by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXdY6mBLCv_B"
   },
   "outputs": [],
   "source": [
    "path = '/content/corgi_image.jpg'\n",
    "img = Image.open(path)\n",
    "img_pre = preprocess(img, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = dinov2_vits14(img_pre['pixel_values'].to(device), output_attentions=True)\n",
    "layer = -1\n",
    "attentions = outputs.attentions[layer] # get the attention map at [layer]\n",
    "attentions = visualize_attention(attentions, img_pre['pixel_values'], 14, device)\n",
    "plot_attention(img, attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3wIW1BCf3B1"
   },
   "source": [
    "# **Question 11**\n",
    "> Modify the ```layer``` variable in the previous cell to visualise the attention maps earlier and later in the network. What general pattern do you observe? Why might the model's pretraining cause this pattern to emerge?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7zOAkamiRHy"
   },
   "source": [
    "Next, we are going to load and visualise the [Oxford Flowers Dataset](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/). This is a small dataset for classifying 102 different flower species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3BImWUXym_nx"
   },
   "outputs": [],
   "source": [
    "# Load Oxford Flowers data.\n",
    "batch_size = 64\n",
    "transform_vis = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: img.convert(\"RGB\")),  # convert to RGB\n",
    "    transforms.ToTensor(), # convert to PyTorch tensor\n",
    "    transforms.Resize(244), # resize while maintaining aspect ratio\n",
    "    transforms.CenterCrop(224) # take a square center crop\n",
    "])\n",
    "preprocess_return_tensor = transforms.Lambda(lambda image: preprocess(image, return_tensors=\"pt\"))\n",
    "flowers_train_data_unnormalized = Flowers102(\n",
    "    '../data', split=\"train\", download=True, transform=transform_vis\n",
    "    )\n",
    "flowers_train_data = Flowers102(\n",
    "    '../data', split=\"train\", download=True, transform=preprocess_return_tensor\n",
    "    )\n",
    "flowers_val_data = Flowers102(\n",
    "    '../data', split=\"val\", download=True, transform=preprocess_return_tensor\n",
    "    )\n",
    "flowers_test_data = Flowers102(\n",
    "    '../data', split=\"test\", download=True, transform=preprocess_return_tensor\n",
    "    )\n",
    "\n",
    "# Visualize data\n",
    "visualization_loader = torch.utils.data.DataLoader(\n",
    "    flowers_train_data_unnormalized,\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Get one batch of images and labels\n",
    "images, labels = next(iter(visualization_loader))\n",
    "\n",
    "# Create a grid of images\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "for i in range(batch_size):\n",
    "    plt.subplot(8, 8, i + 1)  # 8x8 grid\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(images[i].permute(1, 2, 0))  # [C, H, W] → [H, W, C]\n",
    "    plt.title(f\"Label: {labels[i].item()}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w44PeiTWkGr7"
   },
   "source": [
    "# **Question 12**\n",
    "> How many images are in the Oxford Flowers training set? How many images are in the CIFAR10 training set. Given this, what do you think would happen if we tried to train the vision transformer from scratch on Oxford Flowers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hohQwpfmlfed"
   },
   "source": [
    "Instead of training the Dinov2 vision transformer from scratch, we are going to train a linear classifier to predict the flower class from features obtained with the pretrained Dinov2 model. To make training faster, we will pre-compute and save the features from Dinov2 before training our classifier on top. Run the next cell to define the ```compute_embeddings``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLT51udB0_jd"
   },
   "outputs": [],
   "source": [
    "def compute_embeddings(dataset, suffix) -> None:\n",
    "    \"\"\"\n",
    "    Computes DINOv2 embeddings for all images in the dataset and saves them\n",
    "    as an N x 384 dimensional array in an npy file, where N is the number of\n",
    "    images, and 384 is the embedding dimension.\n",
    "\n",
    "    [dataset]: PyTorch dataset containing images to encode with DINOv2\n",
    "    [suffix]: suffix to add to the name of the npy file\n",
    "\n",
    "    Results are saved in the npy file: dinov2_embeddings_[suffix].npy\n",
    "    \"\"\"\n",
    "    embeddings_list = []\n",
    "    with torch.no_grad():\n",
    "        for ind in range(len(dataset)):\n",
    "            image, number = dataset[ind]\n",
    "            outputs = dinov2_vits14(image['pixel_values'].to(device))\n",
    "            last_hidden_states = outputs[0]\n",
    "            cls_token = last_hidden_states[:, 0, :]\n",
    "            embeddings_np = cls_token.squeeze().cpu().numpy()\n",
    "            print(embeddings_np.shape)\n",
    "            embeddings_list.append(embeddings_np)\n",
    "            print(\"Embedded image: \" + str(ind + 1) + \"/\" + str(len(dataset)))\n",
    "\n",
    "    all_embeddings = np.array(embeddings_list)\n",
    "    output_file_name = \"dinov2_embeddings_\" + suffix + \".npy\"\n",
    "    np.save(output_file_name, all_embeddings)\n",
    "    print(\"Saved all Dinov2 embeddings in \" + output_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjohksICmKrM"
   },
   "source": [
    "# **Question 13**\n",
    "> Examine the ```compute_embeddings``` function. What feature vector are we saving for each image and why?\n",
    "\n",
    "Hint: what vector did we use for classification in Part 1? It has a particular name and a particular role..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWcu6OHFnlqC"
   },
   "source": [
    "Run the next cell to compute and save the embeddings for the training, validation, and test images in the Oxford Flowers Dataset. Running this cell may take a couple of minutes. Move on to the next question while you wait for this cell to finish running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ltSRwlv1DcL"
   },
   "outputs": [],
   "source": [
    "# Compute and save the embeddings for the training images.\n",
    "compute_embeddings(flowers_train_data, \"train\")\n",
    "\n",
    "# Compute and save the embeddings for the validation images.\n",
    "compute_embeddings(flowers_val_data, \"val\")\n",
    "\n",
    "# Compute and save the embeddings for the test_images.\n",
    "compute_embeddings(flowers_test_data, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7eSiSgnoG8Z"
   },
   "source": [
    "# **Question 14**\n",
    "> Below we define a ```Dataset``` class for the Oxford Flowers Dataset. In the ```__getitem__``` method, what is ```x_tensor```?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "De-or3Nr4axN"
   },
   "outputs": [],
   "source": [
    "# Define the dataset.\n",
    "class Dinov2Flowers(torch.utils.data.Dataset):\n",
    "    \"\"\"Dinov2 features for Oxford Flowers dataset images.\n",
    "    \"\"\"\n",
    "    def __init__(self, split):\n",
    "      self.dataset = Flowers102('../data', split, download=True)\n",
    "      self.embeddings = np.load(\"dinov2_embeddings_\" + split + \".npy\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_embeddings = self.embeddings[idx, :] / 8.0 # (1, 384)\n",
    "        x_tensor = torch.tensor(img_embeddings / 768.0, dtype=torch.float32)\n",
    "        flower_class = self.dataset[idx][1]\n",
    "        return x_tensor, flower_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6WgFY-RowLS"
   },
   "source": [
    "Below we define a simple linear probe classifier for training on top of the Dinov2 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8aLrxMph6oFk"
   },
   "outputs": [],
   "source": [
    "# Define the classifier.\n",
    "class SimpleClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple linear probe classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.layer1 = nn.Linear(in_features, out_features)\n",
    "        self.activation1 = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dl-lT3IMpoGV"
   },
   "source": [
    "Initialise the classifier and put it on the GPU in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "foR8vlxz6t6K"
   },
   "outputs": [],
   "source": [
    "# Initialize the classifier.\n",
    "# The dimension of the Dinov2 input vector is 384 => [in_features]=384.\n",
    "# Oxford Flowers has 102 classes => [out_features]=102.\n",
    "model = SimpleClassifier(in_features=384, out_features=102)\n",
    "model = model.cuda() # put the model on the GPU for fast training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SE-XMEM74yak"
   },
   "source": [
    "# **Question 15**\n",
    "> Write code to count the number of parameters in the simple classifier and the Dinov2 transformer model respectively. Which model has more parameters? Would it be faster to train the linear probe or the Dinov2 model from scratch and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMnVHM2u59ai"
   },
   "source": [
    "Run the next two cells to define the training and validation functions as well as a function to set the seeds for reproducibility. Setting the seeds ensures rerunning the notebook produces the same results each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDdsBzHk7tgR"
   },
   "outputs": [],
   "source": [
    "# Define training functions.\n",
    "def train(epoch):\n",
    "    loss_avg = 0\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        # This will zero out the gradients for this batch.\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # Calculate the negative log likelihood loss. It is useful to train a classification problem with C classes.\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss_avg += loss.item()\n",
    "        loss.backward()\n",
    "        # Do a one-step update on our parameters.\n",
    "        optimizer.step()\n",
    "        # Print out the loss.\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return loss_avg / len(train_loader.dataset)\n",
    "\n",
    "def val():\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        with torch.inference_mode():\n",
    "            output = model(data)\n",
    "        val_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    print('\\nVal set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        val_loss, correct, len(val_loader.dataset),\n",
    "        100. * correct / len(val_loader.dataset)))\n",
    "    return val_loss, 100. * correct / len(val_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GcBLi5FFrZyj"
   },
   "outputs": [],
   "source": [
    "def set_all_seeds(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Sets seeds for Python random, NumPy, and PyTorch (CPU and GPU)\n",
    "    for full reproducibility.\n",
    "    \"\"\"\n",
    "    # Python\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "\n",
    "    # Ensure deterministic behavior\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    print(f\"[Seed Set] All seeds set to {seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8B6JBeGY64E4"
   },
   "source": [
    "Now we can set the hyperparameters for training our simple classifier. The hyperparameters are ```batch_size```, ```learning_rate```, and ```epochs```. These can be chosen to maximise the accuracy on the validation set. If we had more time, you could rerun the training procedure for different sets of hyperparameters and choose the ones that achieve the best validation accuracy. For now, they are set to the values below after some minimal tuning on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OihRpVLk73Lr"
   },
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility.\n",
    "set_all_seeds(0)\n",
    "# Choose training hyperparameters.\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "epochs = 60\n",
    "\n",
    "# Set up the dataloaders.\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    Dinov2Flowers(split=\"train\"),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    Dinov2Flowers(split=\"val\"),\n",
    "    batch_size=1, shuffle=False)\n",
    "\n",
    "# Set up the optimizer.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8jYGSQY8Bo6"
   },
   "source": [
    "Run the next cell to train the classifier. Training will take around five minutes, and the model should achieve around 94% accuracy on the validation set after training completes. Please work on the next question while you wait for the cell to finish running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "YvyBwhph-Lv_"
   },
   "outputs": [],
   "source": [
    "epoch_list = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_list.append(epoch)\n",
    "    train_loss = train(epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    val_loss, val_acc = val()\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvNS-RMI9aqp"
   },
   "source": [
    "# **Question 16**\n",
    "> Why is it important to use the validation set to determine hyperparameters? Why not use the training or test sets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNQmJ8YyBQBg"
   },
   "source": [
    "Run the next cell to evaluate our model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OqjV7iYum_jY"
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        with torch.inference_mode():\n",
    "            output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss, 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    Dinov2Flowers(split=\"test\"),\n",
    "    batch_size=1, shuffle=False)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeDrGE8rBbT2"
   },
   "source": [
    "# **Question 17**\n",
    "> How well does our model perform on the test set? Assuming that 70% accuracy is considered 'pretty good' on this dataset, are you satisfied with our model's performance? What techniques and strategies did we use that could explain the performance of our model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwKHdkYzEBRJ"
   },
   "source": [
    "#✅**Checkpoint 2**\n",
    "Check your answers so far for Part 2 with a TA before moving on to the rest of the practical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSGniRN7ItQX"
   },
   "source": [
    "In the next section, we will investigate another method of adapting powerful pretrained vision transformers to our specific flower classification task. Unlike in the previous section, we will not need to do any training. Instead, we will use a vision transformer pretrained with [Contrastive Language-Image Pretraining (CLIP)](https://arxiv.org/abs/2103.00020), for 'zero-shot classification.'\n",
    "\n",
    "CLIP is a method that allows for large-scale pretraining of models using weak supervision. Given a large-scale dataset of images and captions describing the images, CLIP trains an image model ('image encoder') and a text model ('text encoder') to map each image to a feature vector and each caption to a feature vector such that images and their matching captions are mapped to vectors that are 'close' to and images and unrelated captions are mapped to vectors that are 'far' away from each other. 'Closeness' and 'farness' are evaluated using the cosine similarity. This training objective is a form of weak supervision, since the texts are not constrained to a particular task and can be easily obtained by collecting images and unstructured captions from the web. Because the data constraints are few, this objective easily scales to millions and billions of image-text pairs.\n",
    "\n",
    "Zero-shot classification means that the CLIP model can be adapted to new classification tasks without being trained on any data samples (i.e., trained on *zero* data samples) for those classification tasks. This is possible because the CLIP training objective is general and allows for the specification of the text descriptions at inference time. Specifically, given a new problem with $C$ classes, we can encode the $C$ classes as $C$ text descriptions with the CLIP text encoder. For any image, we can encode it with the image encoder. Then, using the cosine similarity, we can compare the image CLIP feature vector to the text feature vectors for each class and pick the class that the image is closest to. This is what we will do next for the flower dataset. In our case, the image encoder is a vision transformer, and the text encoder is a text transformer. The image and text transformers were pretrained jointly using the CLIP objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vi5PULj4QtH6"
   },
   "source": [
    "Run the next cell to install the CLIP library from OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1pcBJxuKGy6"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_DQvhviQ0WJ"
   },
   "source": [
    "Run the next cell to import the clip module and see a list of available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukRSrUOI9zYj"
   },
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9ryf6WrSOH2"
   },
   "source": [
    "We will choose the ViT-L/14 model in the next cell, which includes both the pretrained image encoder, a vision transformer with a 14 x 14 patch size, and the text encoder, a text transformer pretrained using CLIP with the image encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huGq7BFg98FJ"
   },
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-L/14\")\n",
    "model.cuda().eval()\n",
    "\n",
    "print(model)\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SGzoLvaWMBk"
   },
   "source": [
    "Next, we will load the Oxford Flowers Dataset again but this time using the CLIP image pre-processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMtVgFhqQHO1"
   },
   "outputs": [],
   "source": [
    "# Get image input.\n",
    "flowers_test_data_vis = Flowers102('../data', split=\"test\", download=True)\n",
    "flowers_test_data = Flowers102('../data', split=\"test\", download=True, transform=preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-hDqJvdXsCk"
   },
   "source": [
    "To get the text input, we will use the flower classes in the Oxford Flowers dataset. Rather than just encoding the classes by themselves, we will place each class into a fixed *prompt template* that adds some context to the text descriptions. Because captions tend to describe the images they correspond to, adding the prompt template makes the flower classes more similar to the text descriptions the model was trained on. This improves the accuracy on our downstream classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGci6soLMd2W"
   },
   "outputs": [],
   "source": [
    "# Get text input\n",
    "flower_text_descriptions = [f\"a photo of a {label}, a type of flower\" for label in flowers_test_data.classes]\n",
    "texts = [label for label in flowers_test_data.classes]\n",
    "text_tokens = clip.tokenize(flower_text_descriptions).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b_VknGadluL"
   },
   "source": [
    "The cosine similarity for two vectors $\\mathbf{v}$ and $\\mathbf{w}$ is the dot product of the two normalised vectors. Letting $|| \\cdot ||$ represent the $\\mathcal{L}_{2}$ norm, we have the following equation for the cosine similarity:\n",
    "\n",
    "$similarity(\\mathbf{v}, \\mathbf{w}) = \\frac{\\mathbf{v}\\cdot \\mathbf{w}}{|| \\mathbf{v} |||| \\mathbf{w}||}$ with\n",
    "\n",
    "$\\mathbf{v}\\cdot \\mathbf{w} = \\sum_{i=1}^{N}v_{i}w_{i}$,\n",
    "\n",
    "where $v_{i}$ is the $i^{th}$ element of $\\mathbf{v}$, and $w_{i}$ is the $i^{th}$ element of $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWnsmeXCw5ci"
   },
   "source": [
    "# **Question 18**\n",
    "> In the below code, the image and text features are computed for the Oxford Flowers test set. Each image is mapped to a vector of length 768, and each text description is mapped to a vector of length 768 in a pretrained joint image-text embedding space where the vectors can be compared with the cosine similarity. Write code in the ```TO DO``` block below to calculate the cosine similarity between the image and text features, and store the result in the ```similarity``` variable. The rest of the code chooses the flower class with the highest cosine similarity as the predicted label for the image. What classification accuracy do you get when you run the code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FfTy-9g0n14b"
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    sample_ind = 1\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        with torch.inference_mode():\n",
    "            # Get the image and text features\n",
    "            image_features = model.encode_image(data).float()\n",
    "            text_features = model.encode_text(text_tokens).float()\n",
    "            print(\"image_features.shape: \" + str(image_features.shape))\n",
    "            print(\"text_features.shape: \" + str(text_features.shape))\n",
    "            # TO DO: Calculate the cosine similarity and save it to [similarity]\n",
    "\n",
    "\n",
    "            pred = similarity.max(1, keepdim=True)[1] # get the index of the max similarity\n",
    "            sample_correct = pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "            print(str(sample_ind) + \"/\" + str(len(test_loader)))\n",
    "            sample_ind += 1\n",
    "            correct += sample_correct\n",
    "\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    flowers_test_data,\n",
    "    batch_size=32, shuffle=False)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCMVLBjv0bRD"
   },
   "source": [
    "# **Question 19**\n",
    "> Are you satisfied with the performance of CLIP on our flower classification task? How does it compare to training the linear probe on top of Dinov2 features? Why do you think one method works better than the other? What could we do to improve the zero-shot performance of CLIP on this task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNEjVqy15oaG"
   },
   "source": [
    "#✅**Checkpoint 3**\n",
    "Congratulations! You have finished the lab! Check your answers to the remaining questions with a TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHTHbJt6pdQj"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this practical, we have investigated the Vision Transformer (ViT) and seen how to best leverage its capabilities for the task of image classification. We implemented and trained a ViT from scratch on the small CIFAR10 dataset as well as adapted vision transformers pretrained on large-scale datasets to flower classification. The code from this lab may be used and adapted for your own projects, so I recommend saving this notebook for future reference.\n",
    "\n",
    "### References\n",
    "\n",
    "Dosovitskiy, Alexey, et al. \"An image is worth 16x16 words: Transformers for image recognition at scale.\" International Conference on Learning Representations (2021). [link](https://arxiv.org/pdf/2010.11929.pdf)\n",
    "\n",
    "Chen, Xiangning, et al. \"When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations.\" arXiv preprint arXiv:2106.01548 (2021). [link](https://arxiv.org/abs/2106.01548)\n",
    "\n",
    "Amini-Naieni, Niki, et al. \"Instant Uncertainty Calibration of NeRFs Using a Meta-Calibrator.\" European Conference on Computer Vision (2024). [link](https://arxiv.org/abs/2312.02350)\n",
    "\n",
    "Oquab, Maxime, et al. \"DINOv2: Learning Robust Visual Features without Supervision.\" International Conference on Machine Learning (2021). [link](https://arxiv.org/abs/2304.07193)\n",
    "\n",
    "Radford, Alec, et al. \"Learning Transferable Visual Models From Natural Language Supervision.\" [link](https://arxiv.org/abs/2103.00020)\n",
    "\n",
    "Nilsback, Maria-Elena, et al. \"Automated Flower Classification over a Large Number of Classes.\" Indian Conference on Computer Vision, Graphics and Image Processing (2008). [link](https://www.robots.ox.ac.uk/~vgg/publications/2008/Nilsback08/)\n",
    "\n",
    "Tolstikhin, Ilya, et al. \"MLP-mixer: An all-MLP Architecture for Vision.\" arXiv preprint arXiv:2105.01601 (2021). [link](https://arxiv.org/abs/2105.01601)\n",
    "\n",
    "Xiong, Ruibin, et al. \"On layer normalization in the transformer architecture.\" International Conference on Machine Learning. PMLR, 2020. [link](http://proceedings.mlr.press/v119/xiong20b/xiong20b.pdf)\n",
    "\n",
    "### Code borrowed and adapted from the following sources\n",
    "\n",
    "[1] https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb\n",
    "\n",
    "[2] https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DINO/Visualize_self_attention_of_DINO.ipynb#scrollTo=4o2nUmAOZ2Bo\n",
    "\n",
    "[3] https://colab.research.google.com/drive/1tRRuT21W3VUvORCFRazrVaFLSWYbYoqL?usp=sharing#scrollTo=Rb5jVjzvpacU\n",
    "\n",
    "[4] https://github.com/TrasperJ/102-flowers-classfication-with-PyTorch/blob/master/102flower_classification.ipynb\n",
    "\n",
    "[5] https://github.com/niki-amini-naieni/instantcalibration\n",
    "\n",
    "[6] https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial15/Vision_Transformer.ipynb"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
